{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d40dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodericktabalba/miniforge3/envs/rag_workshop_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    }
   ],
   "source": [
    "from utils.web_scraper import ai_crawler\n",
    "import textwrap\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import requests\n",
    "import html2text\n",
    "embedding_model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Intel/dynamic_tinybert\")\n",
    "qna_model = AutoModelForQuestionAnswering.from_pretrained(\"Intel/dynamic_tinybert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b2efc",
   "metadata": {},
   "source": [
    "# Tokenization (Not super important, but useful to understand embeddings)\n",
    "\n",
    "How a document is split into subunits.\n",
    "\n",
    "* Character Tokens\n",
    "* Word Tokens\n",
    "* Sentence Tokens\n",
    "\n",
    "### GPT\n",
    "* [Byte-Pair Encoding Tokens](https://towardsdatascience.com/byte-pair-encoding-for-beginners-708d4472c0c7/)\n",
    "\n",
    "![Alt text for the image](https://towardsdatascience.com/wp-content/uploads/2023/10/1tQx4iDNDvME61PGO3t_qAw-768x540.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce236eb",
   "metadata": {},
   "source": [
    "# Text Embedding\n",
    "\n",
    "An embedding is a list of numbers that represents the meaning of text, so similar things have similar numbers.\n",
    "\n",
    "Far apart (different meaning): \"king\" and \"banana\" → vectors are far apart because they have unrelated meanings.\n",
    "\n",
    "Close together (similar meaning): \"king\" and \"queen\" → vectors are near each other because both are royalty.\n",
    "Close together (similar meaning): \"man\" and \"woman\" → vectors are near each other because both describe a gender.\n",
    "\n",
    "Cool Fact: \n",
    "embedding(\"king\") - embedding(\"man\") + embedding(\"woman\") ≈ embedding(\"queen\")\n",
    "\n",
    "![3D Representation of word embeddings](https://ai.engin.umich.edu/wp-content/uploads/sites/8/2020/06/king-queen.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1101e8",
   "metadata": {},
   "source": [
    "# In this Workshop, we will be using sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346d75b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The capital of China is Beijing',\n",
       " 'Gravity is a force that attracts two bodies towards each other and it gives weight to physical objects and is responsible for the movement of planets around the sun']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The queries and documents to embed\n",
    "queries = [\n",
    "    \"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "\n",
    "context = \"The capital of China is Beijing. Gravity is a force that attracts two bodies towards each other and it gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n",
    "\n",
    "\n",
    "documents = [sentence.strip() for sentence in context.split(\".\")[:2]]\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99198d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7527, 0.1588],\n",
      "        [0.1783, 0.5727]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# embedding_model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\") # Commented because we already loaded it above\n",
    "\n",
    "query_embeddings = embedding_model.encode(queries, prompt_name=\"query\")\n",
    "document_embeddings = embedding_model.encode(documents)\n",
    "\n",
    "# Compute the (cosine) similarity between the query and document embeddings\n",
    "similarity = embedding_model.similarity(query_embeddings, document_embeddings)\n",
    "\n",
    "# Print the similarity scores\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76092b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76156157",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bcb44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Herman Melville - Moby-Dick\n",
      "\n",
      "Availing himself of the mild, summer-cool weather that now reigned in these\n",
      "latitudes, and in preparation for the peculiarly active pursuits shortly to be\n",
      "anticipated, Perth, the begrimed, blistered old blacksmith, had not removed\n",
      "his portable forge to the hold again, after concluding his contributory work\n",
      "for Ahab's leg, but still retained it on deck, fast lashed to ringbolts by the\n",
      "foremast; being now almost incessantly invoked by the headsmen, and\n",
      "harpooneers, and bowsmen to do some little job for them; altering, or\n",
      "repairing, or new shaping their various weapons and boat furniture. Often he\n",
      "would be surrounded by an eager circle, all waiting to be served; holding\n",
      "boat-spades, pike-heads, harpoons, and lances, and jealously watching his\n",
      "every sooty movement, as he toiled. Nevertheless, this old man's was a patient\n",
      "hammer wielded by a patient arm. No murmur, no impatience, no petulance did\n",
      "come from him. Silent, slow, and solemn; bowing over still further his\n",
      "chronically broken back, he toiled away, as if toil were life itself, and the\n",
      "heavy beating of his hammer the heavy beating of his heart. And so it\n",
      "was.—Most miserable! A peculiar walk in this old man, a certain slight but\n",
      "painful appearing yawing in his gait, had at an early period of the voyage\n",
      "excited the curiosity of the mariners. And to the importunity of their\n",
      "persisted questionings he had finally given in; and so it came to pass that\n",
      "every one now knew the shameful story of his wretched fate. Belated, and not\n",
      "innocently, one bitter winter's midnight, on the road running between two\n",
      "country towns, the blacksmith half-stupidly felt the deadly numbness stealing\n",
      "over him, and sought refuge in a leaning, dilapidated barn. The issue was, the\n",
      "loss of the extremities of both feet. Out of this revelation, part by part, at\n",
      "last came out the four acts of the gladness, and the one long, and as yet\n",
      "uncatastrophied fifth act of the grief of his life's drama. He was an old man,\n",
      "who, at the age of nearly sixty, had postponedly encountered that thing in\n",
      "sorrow's technicals called ruin. He had been an artisan of famed excellence,\n",
      "and with plenty to do; owned a house and garden; embraced a youthful,\n",
      "daughter-like, loving wife, and three blithe, ruddy children; every Sunday\n",
      "went to a cheerful-looking church, planted in a grove. But one night, under\n",
      "cover of darkness, and further concealed in a most cunning disguisement, a\n",
      "desperate burglar slid into his happy home, and robbed them all of everything.\n",
      "And darker yet to tell, the blacksmith himself did ignorantly conduct this\n",
      "burglar into his family's heart. It was the Bottle Conjuror! Upon the opening\n",
      "of that fatal cork, forth flew the fiend, and shrivelled up his home. Now, for\n",
      "prudent, most wise, and economic reasons, the blacksmith's shop was in the\n",
      "basement of his dwelling, but with a separate entrance to it; so that always\n",
      "had the young and loving healthy wife listened with no unhappy nervousness,\n",
      "but with vigorous pleasure, to the stout ringing of her young-armed old\n",
      "husband's hammer; whose reverberations, muffled by passing through the floors\n",
      "and walls, came up to her, not unsweetly, in her nursery; and so, to stout\n",
      "Labor's iron lullaby, the blacksmith's infants were rocked to slumber. Oh, woe\n",
      "on woe! Oh, Death, why canst thou not sometimes be timely? Hadst thou taken\n",
      "this old blacksmith to thyself ere his full ruin came upon him, then had the\n",
      "young widow had a delicious grief, and her orphans a truly venerable,\n",
      "legendary sire to dream of in their after years; and all of them a care-\n",
      "killing competency.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# URL example\n",
    "# Test with a simple URL that should work\n",
    "# document = ai_crawler(\n",
    "#     start_url=\"https://httpbin.org/html\",  # Simple test HTML page\n",
    "#     num_workers=1,\n",
    "#     num_levels_deep=0  # Only crawl the starting page\n",
    "# )\n",
    "\n",
    "## simple version\n",
    "response = requests.get(\"https://httpbin.org/html\")\n",
    "text = html2text.html2text(response.text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64866e78",
   "metadata": {},
   "source": [
    "# Data Pre-processing (Optional)\n",
    "\n",
    "Some data may include:\n",
    "* tables\n",
    "* images\n",
    "* html\n",
    "* grabled characters (EX: â€™)\n",
    "* lists\n",
    "\n",
    "Which may all require their own special formatter.\n",
    "Here are some tools that are freely available to help:\n",
    "* camelot -- parses tables\n",
    "* datalab-to/marker -- another table parser\n",
    "* pdfminer\n",
    "* pymupdf \n",
    "* BeautifulSoup\n",
    "* Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d597ac95",
   "metadata": {},
   "source": [
    "# Naive Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883fe65",
   "metadata": {},
   "source": [
    "### Indexing (Chunk Size)\n",
    "\n",
    "How should I split my text in to chunks?\n",
    "\n",
    "Note: Also for the sake of simplicity, we are not tokenizing the text.\n",
    "In practice, you would want to tokenize them for better results to avoid splitting words in half\n",
    "But also keep in mind, since we are using a sentence embedder, it would make more sense to split the words by sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1dbf379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: # Herman Melville - Moby-Dick\n",
      "\n",
      "Availing himself of the mild, summer-cool weather that now reigned in these\n",
      "latitudes, and in preparation for the peculiarly active pursuits shortly to be\n",
      "anticipated, P\n",
      "Chunk 2: erth, the begrimed, blistered old blacksmith, had not removed\n",
      "his portable forge to the hold again, after concluding his contributory work\n",
      "for Ahab's leg, but still retained it on deck, fast lashed to\n",
      "Chunk 3:  ringbolts by the\n",
      "foremast; being now almost incessantly invoked by the headsmen, and\n",
      "harpooneers, and bowsmen to do some little job for them; altering, or\n",
      "repairing, or new shaping their various weap\n",
      "Chunk 4: ons and boat furniture. Often he\n",
      "would be surrounded by an eager circle, all waiting to be served; holding\n",
      "boat-spades, pike-heads, harpoons, and lances, and jealously watching his\n",
      "every sooty movemen\n",
      "Chunk 5: t, as he toiled. Nevertheless, this old man's was a patient\n",
      "hammer wielded by a patient arm. No murmur, no impatience, no petulance did\n",
      "come from him. Silent, slow, and solemn; bowing over still furth\n",
      "Chunk 6: er his\n",
      "chronically broken back, he toiled away, as if toil were life itself, and the\n",
      "heavy beating of his hammer the heavy beating of his heart. And so it\n",
      "was.—Most miserable! A peculiar walk in this \n",
      "Chunk 7: old man, a certain slight but\n",
      "painful appearing yawing in his gait, had at an early period of the voyage\n",
      "excited the curiosity of the mariners. And to the importunity of their\n",
      "persisted questionings h\n",
      "Chunk 8: e had finally given in; and so it came to pass that\n",
      "every one now knew the shameful story of his wretched fate. Belated, and not\n",
      "innocently, one bitter winter's midnight, on the road running between t\n",
      "Chunk 9: wo\n",
      "country towns, the blacksmith half-stupidly felt the deadly numbness stealing\n",
      "over him, and sought refuge in a leaning, dilapidated barn. The issue was, the\n",
      "loss of the extremities of both feet. Ou\n",
      "Chunk 10: t of this revelation, part by part, at\n",
      "last came out the four acts of the gladness, and the one long, and as yet\n",
      "uncatastrophied fifth act of the grief of his life's drama. He was an old man,\n",
      "who, at \n",
      "Chunk 11: the age of nearly sixty, had postponedly encountered that thing in\n",
      "sorrow's technicals called ruin. He had been an artisan of famed excellence,\n",
      "and with plenty to do; owned a house and garden; embrace\n",
      "Chunk 12: d a youthful,\n",
      "daughter-like, loving wife, and three blithe, ruddy children; every Sunday\n",
      "went to a cheerful-looking church, planted in a grove. But one night, under\n",
      "cover of darkness, and further conc\n",
      "Chunk 13: ealed in a most cunning disguisement, a\n",
      "desperate burglar slid into his happy home, and robbed them all of everything.\n",
      "And darker yet to tell, the blacksmith himself did ignorantly conduct this\n",
      "burgla\n",
      "Chunk 14: r into his family's heart. It was the Bottle Conjuror! Upon the opening\n",
      "of that fatal cork, forth flew the fiend, and shrivelled up his home. Now, for\n",
      "prudent, most wise, and economic reasons, the bla\n",
      "Chunk 15: cksmith's shop was in the\n",
      "basement of his dwelling, but with a separate entrance to it; so that always\n",
      "had the young and loving healthy wife listened with no unhappy nervousness,\n",
      "but with vigorous ple\n",
      "Chunk 16: asure, to the stout ringing of her young-armed old\n",
      "husband's hammer; whose reverberations, muffled by passing through the floors\n",
      "and walls, came up to her, not unsweetly, in her nursery; and so, to st\n",
      "Chunk 17: out\n",
      "Labor's iron lullaby, the blacksmith's infants were rocked to slumber. Oh, woe\n",
      "on woe! Oh, Death, why canst thou not sometimes be timely? Hadst thou taken\n",
      "this old blacksmith to thyself ere his fu\n",
      "Chunk 18: ll ruin came upon him, then had the\n",
      "young widow had a delicious grief, and her orphans a truly venerable,\n",
      "legendary sire to dream of in their after years; and all of them a care-\n",
      "killing competency.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chunk size\n",
    "X = 200  # characters per chunk\n",
    "\n",
    "# Split into chunks\n",
    "chunks = [text[i:i+X] for i in range(0, len(text), X)]\n",
    "\n",
    "# Print results\n",
    "for idx, chunk in enumerate(chunks, start=1):\n",
    "    print(f\"Chunk {idx}: {chunk}\")\n",
    "\n",
    "## Look at the chunks -- the text is split in the middle of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fee2d0",
   "metadata": {},
   "source": [
    "### Retrieving the chunks\n",
    "\n",
    "How should I retrieved those chunks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a8d23bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0821, 0.1290, 0.0224, 0.1546, 0.1770, 0.2171, 0.2661, 0.0638, 0.3043,\n",
      "         0.0438, 0.1750, 0.0795, 0.0501, 0.1321, 0.1390, 0.1799, 0.0613, 0.0805]])\n",
      "\n",
      "\n",
      "\n",
      "Highest Similarity Score: 0.30\n",
      "\n",
      "\n",
      "wo country towns, the blacksmith half-stupidly felt the deadly numbness stealing\n",
      "over him, and sought refuge in a leaning, dilapidated barn. The issue was, the\n",
      "loss of the extremities of both feet. Ou\n",
      "\n",
      "\n",
      "\n",
      "Lowest Similarity Score: 0.02\n",
      "\n",
      "\n",
      " ringbolts by the foremast; being now almost incessantly invoked by the\n",
      "headsmen, and harpooneers, and bowsmen to do some little job for them; altering,\n",
      "or repairing, or new shaping their various weap\n"
     ]
    }
   ],
   "source": [
    "# Questions:\n",
    "# What work was Perth doing on deck, and why was his forge not stored in the hold?\n",
    "# What physical impairment did Perth have, and how did it occur?\n",
    "# What personal tragedy led to the ruin of Perth’s home and family life?\n",
    "# How is Perth’s hammering described in relation to his life and heart?\n",
    "# What does the narrator suggest would have been a kinder fate for Perth before his ruin?\n",
    "\n",
    "query = \"What physical impairment did Perth have, and how did it occur?\"\n",
    "# query = \"What work was Perth doing on deck, and why was his forge not stored in the hold?\"\n",
    "# qs hammering described in relation to his life and heart?\"\n",
    "# query = \"What does theuery = \"What personal tragedy led to the ruin of Perth’s home and family life?\"\n",
    "# query = \"How is Perth’ narrator suggest would have been a kinder fate for Perth before his ruin?\"\n",
    "\n",
    "chunk_embeddings = embedding_model.encode(chunks, prompt_name=\"query\")\n",
    "query_embedding = embedding_model.encode(query, prompt_name=\"query\")\n",
    "\n",
    "# Compute the (cosine) similarity between the query and document embeddings\n",
    "similarity = embedding_model.similarity(query_embedding, chunk_embeddings)\n",
    "\n",
    "# Print the similarity scores\n",
    "print(similarity)\n",
    "\n",
    "# Print Index of Highest Similarity Score\n",
    "print(f\"\\n\\n\")\n",
    "\n",
    "# Print Highest Similarity Score\n",
    "print(f\"Highest Similarity Score: {similarity.max():.2f}\")\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Print Chunk with Highest Similarity Score\n",
    "highest_rated_chunk = chunks[similarity.argmax()]\n",
    "print(textwrap.fill(highest_rated_chunk, width=80))\n",
    "\n",
    "# Print Index of Lowest Similarity Score\n",
    "print(f\"\\n\\n\")\n",
    "\n",
    "# Print Lowest Similarity Score\n",
    "print(f\"Lowest Similarity Score: {similarity.min():.2f}\")\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Print Chunk with Lowest Similarity Score\n",
    "lowest_rated_chunk = chunks[similarity.argmin()]\n",
    "print(textwrap.fill(lowest_rated_chunk, width=80))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dd2257",
   "metadata": {},
   "source": [
    "### Generating the answer\n",
    "\n",
    "How should I generate the answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6618bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: the loss of the extremities\n",
      "Answer: altering, or repairing\n"
     ]
    }
   ],
   "source": [
    "highest_rated_chunk_tokens = tokenizer.encode_plus(query, highest_rated_chunk, return_tensors=\"pt\", truncation=True)\n",
    "lowest_rated_chunk_tokens = tokenizer.encode_plus(query, lowest_rated_chunk, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "# Highest rated chunk\n",
    "highest_rated_chunk_input_ids = highest_rated_chunk_tokens[\"input_ids\"]\n",
    "highest_rated_chunk_attention_mask = highest_rated_chunk_tokens[\"attention_mask\"]\n",
    "\n",
    "# Perform question answering\n",
    "outputs = qna_model(highest_rated_chunk_input_ids, attention_mask=highest_rated_chunk_attention_mask)\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "# Find the start and end positions of the answer\n",
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores) + 1\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(highest_rated_chunk_input_ids[0][answer_start:answer_end]))\n",
    "print(f\"Answer: {answer}\")\n",
    "\n",
    "# Lowest rated chunk\n",
    "lowest_rated_chunk_input_ids = lowest_rated_chunk_tokens[\"input_ids\"]\n",
    "lowest_rated_chunk_attention_mask = lowest_rated_chunk_tokens[\"attention_mask\"]\n",
    "\n",
    "# Perform question answering\n",
    "outputs = qna_model(lowest_rated_chunk_input_ids, attention_mask=lowest_rated_chunk_attention_mask)\n",
    "## Note here that we are using a qna model to generate the answer -- which is trained on question-answer pairs\n",
    "## General-purpose models may need additonal prompting to generate the answer:\n",
    "## \"You are a helpful assistant, answer the question based on the context provided\"\n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "# Find the start and end positions of the answer\n",
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores) + 1\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(lowest_rated_chunk_input_ids[0][answer_start:answer_end]))\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebafcdd",
   "metadata": {},
   "source": [
    "# GPT-Generated Answers from the text for confirmation\n",
    "\n",
    "1. What physical impairment did Perth have, and how did it occur?\n",
    "\n",
    "Perth lost the extremities of both feet.\n",
    "\n",
    "This occurred one bitter winter's midnight, when he sought refuge in a dilapidated barn and felt deadly numbness stealing over him, ultimately leading to the loss of his feet.\n",
    "\n",
    "2. What work was Perth doing on deck, and why was his forge not stored in the hold?\n",
    "\n",
    "Perth was altering, repairing, or reshaping weapons and boat furniture for the headsmen, harpooneers, and bowsmen.\n",
    "\n",
    "His portable forge was kept on deck, fastened to ringbolts by the foremast, because he was almost constantly asked to do small jobs and needed it readily accessible.\n",
    "\n",
    "3. What personal tragedy led to the ruin of Perth’s home and family life?\n",
    "\n",
    "A burglar, the Bottle Conjuror, broke into his home under cover of darkness.\n",
    "\n",
    "Perth unknowingly led the burglar into his family's home, resulting in the loss of everything he owned—his house, garden, and possessions—leaving his family destitute.\n",
    "\n",
    "4. How is Perth’s hammering described in relation to his life and heart?\n",
    "\n",
    "Perth’s hammering is described as patient, slow, solemn, and almost lifelike, with the heavy beating of his hammer likened to the heavy beating of his heart.\n",
    "\n",
    "It conveys his dedication, endurance, and the sense that toil itself is the essence of his life.\n",
    "\n",
    "5. What does the narrator suggest would have been a kinder fate for Perth before his ruin?\n",
    "\n",
    "The narrator suggests that Death taking Perth before his full ruin would have been kinder, so that his young widow and orphans would have experienced grief without total destitution and retained the memory of a “venerable, legendary sire” rather than enduring complete loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615abfc",
   "metadata": {},
   "source": [
    "# LLM alternative\n",
    "\n",
    "If your lab has access to OpenAI, here is the code for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe4483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "# # Initialize client\n",
    "# client = OpenAI(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "# # Your query and chunks\n",
    "# query = \"What physical impairment did Perth have, and how did it occur?\"\n",
    "\n",
    "# def get_llm_answer(question, context):\n",
    "#     prompt = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         temperature=0  # deterministic answers\n",
    "#     )\n",
    "#     answer = response.choices[0].message.content.strip()\n",
    "#     return answer\n",
    "\n",
    "# # Highest rated chunk\n",
    "# highest_answer = get_llm_answer(query, highest_rated_chunk)\n",
    "# print(f\"Answer from highest rated chunk: {highest_answer}\")\n",
    "\n",
    "# # Lowest rated chunk\n",
    "# lowest_answer = get_llm_answer(query, lowest_rated_chunk)\n",
    "# print(f\"Answer from lowest rated chunk: {lowest_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41bbe058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, context):\n",
    "    chunk_tokens = tokenizer.encode_plus(question, context, return_tensors=\"pt\", truncation=True)\n",
    "    chunk_input_ids = chunk_tokens[\"input_ids\"]\n",
    "    chunk_attention_mask = chunk_tokens[\"attention_mask\"]\n",
    "    outputs = qna_model(chunk_input_ids, attention_mask=chunk_attention_mask)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(chunk_input_ids[0][answer_start:answer_end]))\n",
    "    return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "610ba88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9791331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client(Settings(persist_directory=\".chromadb\"))\n",
    "\n",
    "collection = client.create_collection(name=\"my_qa_collection\")\n",
    "\n",
    "collection.add(\n",
    "    documents=chunks,\n",
    "    metadatas=[{\"source\": f\"chunk{i}\"} for i in range(len(chunks))],\n",
    "    ids=[str(i) for i in range(len(chunks))],\n",
    "    embeddings=chunk_embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e46f708c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top match document: erth, the begrimed, blistered old blacksmith, had not removed his portable forge to the hold again, after concluding his contributory work for Ahab's leg, but still retained it on deck, fast lashed to\n",
      "Top match metadata: {'source': 'chunk1'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'contributory work'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"What work was Perth doing on deck, and why was his forge not stored in the hold?\"\n",
    "# query = \"What physical impairment did Perth have, and how did it occur?\"\n",
    "# query = \"What personal tragedy led to the ruin of Perth’s home and family life?\"\n",
    "# query = \"How is Perth’s hammering described in relation to his life and heart?\"\n",
    "# query = \"What does the narrator suggest would have been a kinder fate for Perth before his ruin?\"\n",
    "\n",
    "\n",
    "\n",
    "query_embedding = embedding_model.encode(query, prompt_name=\"query\")\n",
    "results = collection.query(query_embeddings=[query_embedding], n_results=1)\n",
    "\n",
    "print(\"Top match document:\", results['documents'][0][0])\n",
    "print(\"Top match metadata:\", results['metadatas'][0][0])\n",
    "\n",
    "generate_answer(query, results['documents'][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93984c8b",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "How do I know if my RAG is performing correctly?\n",
    "\n",
    "\n",
    "\n",
    "### Option 1 (best case scenario, labeled dataset)\n",
    "\n",
    "You have a labeled dataset, for example [FAQ](https://faq.caesarstone.co.uk/en/care-maintenance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f992acb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What work was Perth doing on deck, and why was his forge not stored in the hold?\n",
      "Answer: Perth was altering, repairing, or reshaping weapons and boat furniture for the headsmen, harpooneers, and bowsmen. His portable forge was kept on deck, fastened to ringbolts by the foremast, because he was almost constantly asked to do small jobs and needed it readily accessible.\n",
      "Generated Answer: contributory work\n",
      "Retrieved Chunk: erth, the begrimed, blistered old blacksmith, had not removed his portable forge to the hold again, after concluding his contributory work for Ahab's leg, but still retained it on deck, fast lashed to\n",
      "\n",
      "\n",
      "Question: What physical impairment did Perth have, and how did it occur?\n",
      "Answer: Perth lost the extremities of both feet. This occurred one bitter winter's midnight, when he sought refuge in a dilapidated barn and felt deadly numbness stealing over him, ultimately leading to the loss of his feet.\n",
      "Generated Answer: the loss of the extremities\n",
      "Retrieved Chunk: wo country towns, the blacksmith half-stupidly felt the deadly numbness stealing over him, and sought refuge in a leaning, dilapidated barn. The issue was, the loss of the extremities of both feet. Ou\n",
      "\n",
      "\n",
      "Question: What personal tragedy led to the ruin of Perth’s home and family life?\n",
      "Answer: A burglar, the Bottle Conjuror, broke into his home under cover of darkness. Perth unknowingly led the burglar into his family's home, resulting in the loss of everything he owned—his house, garden, and possessions—leaving his family destitute.\n",
      "Generated Answer: \n",
      "Retrieved Chunk: the age of nearly sixty, had postponedly encountered that thing in sorrow's technicals called ruin. He had been an artisan of famed excellence, and with plenty to do; owned a house and garden; embrace\n",
      "\n",
      "\n",
      "Question: How is Perth’s hammering described in relation to his life and heart?\n",
      "Answer: Perth’s hammering is described as patient, slow, solemn, and almost lifelike, with the heavy beating of his hammer likened to the heavy beating of his heart. It conveys his dedication, endurance, and the sense that toil itself is the essence of his life.\n",
      "Generated Answer: \n",
      "Retrieved Chunk: er his chronically broken back, he toiled away, as if toil were life itself, and the heavy beating of his hammer the heavy beating of his heart. And so it was.—Most miserable! A peculiar walk in this \n",
      "\n",
      "\n",
      "Question: What does the narrator suggest would have been a kinder fate for Perth before his ruin?\n",
      "Answer: The narrator suggests that Death taking Perth before his full ruin would have been kinder, so that his young widow and orphans would have experienced grief without total destitution and retained the memory of a “venerable, legendary sire” rather than enduring complete loss.\n",
      "Generated Answer: \n",
      "Retrieved Chunk: the age of nearly sixty, had postponedly encountered that thing in sorrow's technicals called ruin. He had been an artisan of famed excellence, and with plenty to do; owned a house and garden; embrace\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qna_data = {\n",
    "  \"questions_and_answers\": [\n",
    "    {\n",
    "      \"question\": \"What work was Perth doing on deck, and why was his forge not stored in the hold?\",\n",
    "      \"answer\": \"Perth was altering, repairing, or reshaping weapons and boat furniture for the headsmen, harpooneers, and bowsmen. His portable forge was kept on deck, fastened to ringbolts by the foremast, because he was almost constantly asked to do small jobs and needed it readily accessible.\"\n",
    "    },\n",
    "    {\n",
    "      \"question\": \"What physical impairment did Perth have, and how did it occur?\",\n",
    "      \"answer\": \"Perth lost the extremities of both feet. This occurred one bitter winter's midnight, when he sought refuge in a dilapidated barn and felt deadly numbness stealing over him, ultimately leading to the loss of his feet.\"\n",
    "    },\n",
    "    {\n",
    "      \"question\": \"What personal tragedy led to the ruin of Perth’s home and family life?\",\n",
    "      \"answer\": \"A burglar, the Bottle Conjuror, broke into his home under cover of darkness. Perth unknowingly led the burglar into his family's home, resulting in the loss of everything he owned—his house, garden, and possessions—leaving his family destitute.\"\n",
    "    },\n",
    "    {\n",
    "      \"question\": \"How is Perth’s hammering described in relation to his life and heart?\",\n",
    "      \"answer\": \"Perth’s hammering is described as patient, slow, solemn, and almost lifelike, with the heavy beating of his hammer likened to the heavy beating of his heart. It conveys his dedication, endurance, and the sense that toil itself is the essence of his life.\"\n",
    "    },\n",
    "    {\n",
    "      \"question\": \"What does the narrator suggest would have been a kinder fate for Perth before his ruin?\",\n",
    "      \"answer\": \"The narrator suggests that Death taking Perth before his full ruin would have been kinder, so that his young widow and orphans would have experienced grief without total destitution and retained the memory of a “venerable, legendary sire” rather than enduring complete loss.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "for chunk, qna in zip(chunks, qna_data[\"questions_and_answers\"]):\n",
    "  question_embedding = embedding_model.encode(qna[\"question\"], prompt_name=\"query\")\n",
    "  retrieved_chunk = collection.query(query_embeddings=[question_embedding], n_results=1)\n",
    "  answer = generate_answer(qna[\"question\"], retrieved_chunk['documents'][0][0])\n",
    "  print(f\"Question: {qna['question']}\")\n",
    "  print(f\"Answer: {qna['answer']}\")\n",
    "  print(f\"Generated Answer: {answer}\")\n",
    "  print(f\"Retrieved Chunk: {retrieved_chunk['documents'][0][0]}\")\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61775989",
   "metadata": {},
   "source": [
    "# More Evaluations\n",
    "\n",
    "### Option 2 (LLMs as a judge)\n",
    "\n",
    "\"Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans.\"\n",
    "\n",
    "(Zheng et al., 2023; [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf); >5000 citations) -- makers of Chatbot Arena\n",
    "\n",
    "[Trulens](https://www.snowflake.com/en/engineering-blog/benchmarking-LLM-as-a-judge-RAG-triad-metrics/)\n",
    "\n",
    "```python\n",
    "prompt: \"\"\"\n",
    "You are a Biologist grader; providing the reasonablness of a given RESPONSE to a give PROMPT.\n",
    "\n",
    "Answer Criteria:\n",
    "{context} ## Where you may or may not have a RAG system to annotate your data\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Option 3 (Synthetic dataset)\n",
    "\n",
    "Generate your own synthetic dataset\n",
    "\n",
    "```python\n",
    "queries = [...,...,...]\n",
    "chunks = [...,...,...]\n",
    "evaluations = []\n",
    "\n",
    "for query, chunk in zip(queries, chunks):\n",
    "    result = generate_answer(query, chunk)\n",
    "    evaluations.append({\"question\": query, \"answer\": result, \"context\": chunk})\n",
    "\n",
    "{\n",
    "    question: \"...\",\n",
    "    answer: \"...\",\n",
    "    context: \"...\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0db63183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Herman Melville - Moby-Dick  Availing himself of the mild, summer-cool weather\n",
      "that now reigned in these latitudes, and in preparation for the peculiarly\n",
      "active pursuits shortly to be anticipated, Perth, the begrimed, blistered old\n",
      "blacksmith, had not removed his portable forge to the hold again, after\n",
      "concluding his contributory work for Ahab's leg, but still retained it on deck,\n",
      "fast lashed to ringbolts by the foremast; being now almost incessantly invoked\n",
      "by the headsmen, and harpooneers, and bowsmen to do some little job for them;\n",
      "altering, or repairing, or new shaping their various weapons and boat furniture.\n",
      "Often he would be surrounded by an eager circle, all waiting to be served;\n",
      "holding boat-spades, pike-heads, harpoons, and lances, and jealously watching\n",
      "his every sooty movement, as he toiled. Nevertheless, this old man's was a\n",
      "patient hammer wielded by a patient arm. No murmur, no impatience, no petulance\n",
      "did come from him. Silent, slow, and solemn; bowing over still further his\n",
      "chronically broken back, he toiled away, as if toil were life itself, and the\n",
      "heavy beating of his hammer the heavy beating of his heart. And so it was.—Most\n",
      "miserable! A peculiar walk in this old man, a certain slight but painful\n",
      "appearing yawing in his gait, had at an early period of the voyage excited the\n",
      "curiosity of the mariners. And to the importunity of their persisted\n",
      "questionings he had finally given in; and so it came to pass that every one now\n",
      "knew the shameful story of his wretched fate. Belated, and not innocently, one\n",
      "bitter winter's midnight, on the road running between two country towns, the\n",
      "blacksmith half-stupidly felt the deadly numbness stealing over him, and sought\n",
      "refuge in a leaning, dilapidated barn. The issue was, the loss of the\n",
      "extremities of both feet. Out of this revelation, part by part, at last came out\n",
      "the four acts of the gladness, and the one long, and as yet uncatastrophied\n",
      "fifth act of the grief of his life's drama. He was an old man, who, at the age\n",
      "of nearly sixty, had postponedly encountered that thing in sorrow's technicals\n",
      "called ruin. He had been an artisan of famed excellence, and with plenty to do;\n",
      "owned a house and garden; embraced a youthful, daughter-like, loving wife, and\n",
      "three blithe, ruddy children; every Sunday went to a cheerful-looking church,\n",
      "planted in a grove. But one night, under cover of darkness, and further\n",
      "concealed in a most cunning disguisement, a desperate burglar slid into his\n",
      "happy home, and robbed them all of everything. And darker yet to tell, the\n",
      "blacksmith himself did ignorantly conduct this burglar into his family's heart.\n",
      "It was the Bottle Conjuror! Upon the opening of that fatal cork, forth flew the\n",
      "fiend, and shrivelled up his home. Now, for prudent, most wise, and economic\n",
      "reasons, the blacksmith's shop was in the basement of his dwelling, but with a\n",
      "separate entrance to it; so that always had the young and loving healthy wife\n",
      "listened with no unhappy nervousness, but with vigorous pleasure, to the stout\n",
      "ringing of her young-armed old husband's hammer; whose reverberations, muffled\n",
      "by passing through the floors and walls, came up to her, not unsweetly, in her\n",
      "nursery; and so, to stout Labor's iron lullaby, the blacksmith's infants were\n",
      "rocked to slumber. Oh, woe on woe! Oh, Death, why canst thou not sometimes be\n",
      "timely? Hadst thou taken this old blacksmith to thyself ere his full ruin came\n",
      "upon him, then had the young widow had a delicious grief, and her orphans a\n",
      "truly venerable, legendary sire to dream of in their after years; and all of\n",
      "them a care-killing competency.\n"
     ]
    }
   ],
   "source": [
    "#Print text\n",
    "print(textwrap.fill(text, width=80) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "206ab975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "\n",
    "# response = ollama.generate(model='gemma:2b', prompt='what is a qubit')\n",
    "\n",
    "# print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b052fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import utils.get_data\n",
    "\n",
    "\n",
    "class QADocumentRetriever:\n",
    "    def __init__(self, documents, chunk_size=200, overlap=0, embedding_model=None, tokenizer=None, llm=None):\n",
    "        self.documents = documents\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        if self.overlap >= self.chunk_size:\n",
    "            raise ValueError(\"Overlap must be less than chunk size\")\n",
    "        \n",
    "        # Chunk documents\n",
    "        self.chunks = self._chunk_documents(documents)\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        self.embedding_model = embedding_model\n",
    "        self.chunk_embeddings = self.embedding_model.encode(self.chunks, batch_size=2, show_progress_bar=True, convert_to_numpy=True).astype(np.float32)\n",
    "        \n",
    "        # Initialize QA model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.qna_model = llm\n",
    "        \n",
    "        # Initialize ChromaDB client and collection\n",
    "        self.client = chromadb.Client(Settings(persist_directory=\".chromadb\"))\n",
    "        existing_collections = [c.name for c in self.client.list_collections()]\n",
    "        if \"qa_collection\" in existing_collections:\n",
    "            self.client.delete_collection(name=\"qa_collection\")\n",
    "        self.collection = self.client.create_collection(name=\"qa_collection\")\n",
    "        self.collection.add(\n",
    "            documents=self.chunks,\n",
    "            metadatas=[{\"source\": f\"chunk{i}\"} for i in range(len(self.chunks))],\n",
    "            ids=[str(i) for i in range(len(self.chunks))],\n",
    "            embeddings=self.chunk_embeddings\n",
    "        )\n",
    "        \n",
    "    def _chunk_documents(self, documents):\n",
    "        chunks = []\n",
    "        for doc in documents:\n",
    "            words = doc.split()\n",
    "            for i in range(0, len(words), self.chunk_size - self.overlap):\n",
    "                chunk = \" \".join(words[i:i+self.chunk_size])\n",
    "                if chunk:  # avoid empty chunks\n",
    "                    chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def generate_answer(self, question, context):\n",
    "        chunk_tokens = self.tokenizer.encode_plus(question, context, return_tensors=\"pt\", truncation=True)\n",
    "        chunk_input_ids = chunk_tokens[\"input_ids\"]\n",
    "        chunk_attention_mask = chunk_tokens[\"attention_mask\"]\n",
    "        outputs = self.qna_model(chunk_input_ids, attention_mask=chunk_attention_mask)\n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores) + 1\n",
    "        answer = self.tokenizer.convert_tokens_to_string(\n",
    "            self.tokenizer.convert_ids_to_tokens(chunk_input_ids[0][answer_start:answer_end])\n",
    "        )\n",
    "        return answer\n",
    "\n",
    "    def retrieve_and_answer(self, question, n_results=1):\n",
    "        # Get embedding for the question\n",
    "        question_embedding = self.embedding_model.encode(question, convert_to_numpy=True)\n",
    "        \n",
    "        # Retrieve top n_results chunks\n",
    "        retrieved = self.collection.query(query_embeddings=[question_embedding], n_results=n_results)\n",
    "        retrieved_chunks = retrieved['documents'][0]\n",
    "        \n",
    "        # Concatenate all retrieved chunks into a single context\n",
    "        full_context = \"\\n\".join(retrieved_chunks)\n",
    "        \n",
    "        # Generate answer using the entire context\n",
    "        answer = self.generate_answer(question, full_context)\n",
    "        \n",
    "        return [{\"chunk\": full_context, \"answer\": answer}]\n",
    "\n",
    "    def evaluate_similarity(self, question):\n",
    "        question_embedding = self.embedding_model.encode(question, convert_to_numpy=True)\n",
    "        similarities = [dot(question_embedding, chunk_emb)/ (norm(question_embedding) * norm(chunk_emb))\n",
    "                        for chunk_emb in self.chunk_embeddings]\n",
    "        return similarities\n",
    "    \n",
    "    def evaluate_qna_set(self, qna_data, n_results=1, print_results=True):\n",
    "        \"\"\"\n",
    "        Evaluate a list of questions and reference answers.\n",
    "        Returns a list of dicts with question, reference answer, generated answer,\n",
    "        retrieved chunk, and similarity score.\n",
    "        \"\"\"\n",
    "        evaluation_results = []\n",
    "\n",
    "        for item in qna_data['questions_and_answers']:\n",
    "            question = item[\"question\"]\n",
    "            reference_answer = item[\"answer\"]\n",
    "\n",
    "            # Retrieve document(s) and generate answer\n",
    "            answers = self.retrieve_and_answer(question, n_results=n_results)\n",
    "            generated = answers[0][\"answer\"]\n",
    "            retrieved_chunk = answers[0][\"chunk\"]\n",
    "\n",
    "            # Compute similarity between generated answer and reference answer\n",
    "            gen_embedding = self.embedding_model.encode(generated, convert_to_numpy=True)\n",
    "            ref_embedding = self.embedding_model.encode(reference_answer, convert_to_numpy=True)\n",
    "            similarity = float(dot(gen_embedding, ref_embedding) / (norm(gen_embedding) * norm(ref_embedding)))\n",
    "\n",
    "            result = {\n",
    "                \"question\": question,\n",
    "                \"reference_answer\": reference_answer,\n",
    "                \"generated_answer\": generated,\n",
    "                \"retrieved_chunk\": retrieved_chunk,\n",
    "                \"similarity_score\": similarity,\n",
    "            }\n",
    "            evaluation_results.append(result)\n",
    "\n",
    "            if print_results:\n",
    "                print(f\"Question: {question}\")\n",
    "                print(f\"Reference Answer: {reference_answer}\")\n",
    "                print(f\"Generated Answer: {generated}\")\n",
    "                print(f\"Similarity Score: {similarity:.4f}\")\n",
    "                print(\"Retrieved Chunk:\\n\", textwrap.fill(retrieved_chunk, width=80))\n",
    "                print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "        avg_similarity = sum(r[\"similarity_score\"] for r in evaluation_results) / len(evaluation_results)\n",
    "        print(\"Average similarity score: \", avg_similarity)\n",
    "\n",
    "        return evaluation_results\n",
    "\n",
    "###### QA Models #######\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"twmkn9/bert-base-uncased-squad2\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"twmkn9/bert-base-uncased-squad2\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ahotrod/electra_large_discriminator_squad2_512\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"ahotrod/electra_large_discriminator_squad2_512\")\n",
    "\n",
    "###### Embedding Models #######\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# embedding_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# embedding_model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "\n",
    "# embedding_model = SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-v4\")\n",
    "\n",
    "# embedding_model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "# embedding_model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\")\n",
    "\n",
    "# embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "qa_retriever = QADocumentRetriever([text], chunk_size=30, overlap=10, embedding_model=embedding_model, tokenizer=tokenizer, llm=model)\n",
    "results = qa_retriever.evaluate_qna_set(qna_data, n_results=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_workshop_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
