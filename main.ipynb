{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b8dfb3",
   "metadata": {},
   "source": [
    "# Personal Running Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d49e9a7",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537c888",
   "metadata": {},
   "source": [
    "### Scanning documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quiet, progress-friendly PDF loader ---\n",
    "import time, logging, warnings, contextlib, io\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) Silence pypdf chatter/warnings\n",
    "try:\n",
    "    from pypdf.errors import PdfReadWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=PdfReadWarning)\n",
    "except Exception:\n",
    "    pass  # older pypdf versions may not expose PdfReadWarning\n",
    "\n",
    "logging.getLogger(\"pypdf\").setLevel(logging.ERROR)\n",
    "\n",
    "# 2) Load PDFs with a progress bar (no ipywidgets required)\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_dir = Path(\"data\")\n",
    "pdf_paths = sorted(pdf_dir.rglob(\"*.pdf\"))\n",
    "\n",
    "docs = []\n",
    "start = time.perf_counter()\n",
    "\n",
    "print(f\"Scanning {len(pdf_paths)} PDFs in {pdf_dir.resolve()} ...\")\n",
    "for p in tqdm(pdf_paths, desc=\"Loading PDFs\", unit=\"file\"):\n",
    "    try:\n",
    "        # Some libraries print to stderr; swallow it to keep the notebook clean\n",
    "        with contextlib.redirect_stderr(io.StringIO()):\n",
    "            loader = PyPDFLoader(str(p))\n",
    "            docs.extend(loader.load())\n",
    "    except Exception as e:\n",
    "        print(f\":warning: Skipped {p.name}: {e}\")\n",
    "\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Done. Loaded {len(docs)} document chunks from {len(pdf_paths)} PDF files in {elapsed:,.1f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15331941",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split (unchanged) ---\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "print(f\"Chunks after splitting: {len(split_docs)}\")\n",
    "\n",
    "# --- Embeddings (new import path) ---\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}  # IP == cosine\n",
    ")\n",
    "\n",
    "# --- Batched FAISS builder: ensures contiguous float32 array ---\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "\n",
    "INDEX_DIR = Path(\"indexes/campus-faiss-batched\")\n",
    "\n",
    "def build_faiss_index_batched(docs, embed, batch_size=1000):\n",
    "    \"\"\"Embed in batches and build a FAISS (IP) index from normalized vectors.\"\"\"\n",
    "    texts  = [d.page_content for d in docs]\n",
    "    metas  = [d.metadata     for d in docs]\n",
    "\n",
    "    all_embeds = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\", unit=\"batch\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        # returns a list of vectors\n",
    "        batch_embeds = embed.embed_documents(batch)\n",
    "        all_embeds.extend(batch_embeds)\n",
    "\n",
    "    # ---> the important part: stack to contiguous float32 array\n",
    "    embs = np.vstack(all_embeds).astype(\"float32\", copy=False)\n",
    "    # (optional sanity checks)\n",
    "    assert embs.ndim == 2 and embs.shape[0] == len(texts), f\"Bad shape: {embs.shape}\"\n",
    "    # if you didn't normalize in the embedder, uncomment the next line:\n",
    "    # faiss.normalize_L2(embs)\n",
    "\n",
    "    dim = embs.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)  # cosine == inner-product on normalized vectors\n",
    "    print(type(embs), embs.dtype, embs.shape, embs.flags['C_CONTIGUOUS'])\n",
    "    index.add(embs)                 # must be contiguous float32\n",
    "\n",
    "    # wrap into LangChain's FAISS store\n",
    "    ids = [str(uuid4()) for _ in range(len(docs))]\n",
    "    docstore = InMemoryDocstore({ids[i]: docs[i] for i in range(len(docs))})\n",
    "    id_map   = {i: ids[i] for i in range(len(docs))}\n",
    "    db = FAISS(embedding_function=embed, index=index,\n",
    "               docstore=docstore, index_to_docstore_id=id_map)\n",
    "    return db\n",
    "\n",
    "def save_index(db, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    db.save_local(str(path))\n",
    "\n",
    "def load_or_build_index(split_docs, embeddings, index_dir: Path):\n",
    "    if index_dir.exists():\n",
    "        try:\n",
    "            print(f\"Loading existing FAISS index from: {index_dir}\")\n",
    "            return FAISS.load_local(\n",
    "                str(index_dir),\n",
    "                embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to load existing index ({e}). Rebuilding...\")\n",
    "\n",
    "    print(\"No existing index found. Building FAISS index…\")\n",
    "    db = build_faiss_index_batched(split_docs, embeddings, batch_size=1000)\n",
    "    save_index(db, index_dir)\n",
    "    print(f\"Saved FAISS index to: {index_dir}\")\n",
    "    return db\n",
    "\n",
    "db = load_or_build_index(split_docs, embeddings, INDEX_DIR)\n",
    "print(\"Vector store ready ✅\")\n",
    "print(f\"Index contains {db.index.ntotal:,} vectors.\")\n",
    "\n",
    "# --- Retriever (choose one) ---\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})\n",
    "# retriever = db.as_retriever(search_type=\"mmr\",\n",
    "#                             search_kwargs={\"k\": 8, \"fetch_k\": 50, \"lambda_mult\": 0.5})\n",
    "\n",
    "# --- QA chain (HF example; swap if needed) ---\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-base\",\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 512}\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"Pipeline ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9b7bd",
   "metadata": {},
   "source": [
    "### Smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Split into chunks of ~1000 characters with 200-character overlap\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total chunks after splitting: {len(split_docs)}\")\n",
    "print(\"\\n--- Preview of first chunk ---\\n\")\n",
    "print(split_docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5a826",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ad083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a8223",
   "metadata": {},
   "source": [
    "### Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(split_docs, embeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f625c",
   "metadata": {},
   "source": [
    "## Test Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e03fe",
   "metadata": {},
   "source": [
    "### UC1: Find Particular Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39568f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where is the CAPP final report from 2024?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"\\n--- UC1: Find Particular Documents ---\\n\")\n",
    "print(\"Response:\", result[\"result\"], \"\\n\")\n",
    "\n",
    "print(\"Documents used:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"{doc.metadata.get('source', 'unknown')} \\n{doc.page_content[:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84198f",
   "metadata": {},
   "source": [
    "UC2: Summarize Particular Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0925a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize the CAPP final report from 2024\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"\\n--- UC2: Summarize Particular Documents ---\\n\")\n",
    "print(\"Summary:\", result[\"result\"], \"\\n\")\n",
    "\n",
    "print(\"Documents used:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"{doc.metadata.get('source', 'unknown')}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9dd0c2",
   "metadata": {},
   "source": [
    "UC3: Find Documents by Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Find documents related to system executive policies on AI\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"\\n--- UC3: Find Documents by Contents ---\\n\")\n",
    "print(\"Response:\", result[\"result\"], \"\\n\")\n",
    "\n",
    "print(\"Documents used:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"{doc.metadata.get('source', 'unknown')}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df412962",
   "metadata": {},
   "source": [
    "UC4: Finding Particular Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When were votes on AI policies conducted?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"\\n--- UC4: Finding Particular Information ---\\n\")\n",
    "print(\"Response:\", result[\"result\"], \"\\n\")\n",
    "\n",
    "print(\"Documents used:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"{doc.metadata.get('source', 'unknown')}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c527360",
   "metadata": {},
   "source": [
    "UC5: Finding Related Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Show me the history of resolutions on GE\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"\\n--- UC5: Finding Related Information ---\\n\")\n",
    "print(\"Response:\", result[\"result\"], \"\\n\")\n",
    "\n",
    "print(\"Documents used:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"{doc.metadata.get('source', 'unknown')}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a986ea3",
   "metadata": {},
   "source": [
    "UC6: Refinement of Found Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"In the history of resolutions you showed me, which ones are supportive or opposing GE reforms?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"\\n--- UC6: Refinement of Found Information ---\\n\")\n",
    "print(\"Response:\", result[\"result\"], \"\\n\")\n",
    "\n",
    "print(\"Documents used:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"{doc.metadata.get('source', 'unknown')}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_workshop_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
